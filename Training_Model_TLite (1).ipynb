{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4247cbdf-821e-4580-934c-1508d69ca185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 253 images belonging to 2 classes.\n",
      "Found 62 images belonging to 2 classes.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abudh\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.5766 - loss: 4.8166\n",
      "Epoch 1: saving model to C:\\Users\\abudh\\Desktop\\CropWatch\\Training_model_epoch_13_0801.weights.h5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.69010, saving model to C:\\Users\\abudh\\Desktop\\CropWatch\\best_weights_13_08.weights.h5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m539s\u001b[0m 7s/step - accuracy: 0.5766 - loss: 4.8138 - val_accuracy: 0.7419 - val_loss: 0.6901 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - accuracy: 0.6224 - loss: 3.1782\n",
      "Epoch 2: saving model to C:\\Users\\abudh\\Desktop\\CropWatch\\Training_model_epoch_13_0802.weights.h5\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.69010\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 9s/step - accuracy: 0.6218 - loss: 3.1884 - val_accuracy: 0.5645 - val_loss: 2.0763 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.6330 - loss: 2.6091\n",
      "Epoch 3: saving model to C:\\Users\\abudh\\Desktop\\CropWatch\\Training_model_epoch_13_0803.weights.h5\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.69010\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 6s/step - accuracy: 0.6330 - loss: 2.6055 - val_accuracy: 0.5645 - val_loss: 3.6932 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.6831 - loss: 2.3550\n",
      "Epoch 4: saving model to C:\\Users\\abudh\\Desktop\\CropWatch\\Training_model_epoch_13_0804.weights.h5\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.69010\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 8s/step - accuracy: 0.6822 - loss: 2.3508 - val_accuracy: 0.2742 - val_loss: 0.8960 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298s/step - accuracy: 0.6551 - loss: 1.0281  \n",
      "Epoch 5: saving model to C:\\Users\\abudh\\Desktop\\CropWatch\\Training_model_epoch_13_0805.weights.h5\n",
      "\n",
      "Epoch 5: val_loss improved from 0.69010 to 0.67219, saving model to C:\\Users\\abudh\\Desktop\\CropWatch\\best_weights_13_08.weights.h5\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18796s\u001b[0m 298s/step - accuracy: 0.6560 - loss: 1.0273 - val_accuracy: 0.5645 - val_loss: 0.6722 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110s/step - accuracy: 0.6096 - loss: 1.1885  \n",
      "Epoch 6: saving model to C:\\Users\\abudh\\Desktop\\CropWatch\\Training_model_epoch_13_0806.weights.h5\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.67219\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6944s\u001b[0m 110s/step - accuracy: 0.6103 - loss: 1.1879 - val_accuracy: 0.5645 - val_loss: 1.2195 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m21/64\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:14\u001b[0m 6s/step - accuracy: 0.7398 - loss: 0.6816"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow.lite as tflite\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "# Parameters\n",
    "dataset_url = r'C:\\Users\\abudh\\Desktop\\CropWatch\\Test5'\n",
    "batch_size = 4\n",
    "img_height = int(2160 / 4)\n",
    "img_width = int(1620 / 4)\n",
    "\n",
    "validation_split = 0.2\n",
    "rescale = 1.0 / 255\n",
    "no_epochs = 100\n",
    "\n",
    "# Data Augmentation and Loading\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=rescale,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    channel_shift_range=0.2,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=validation_split\n",
    ")\n",
    "\n",
    "train_dataset = datagen.flow_from_directory(\n",
    "    directory=dataset_url,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"training\",\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_dataset = datagen.flow_from_directory(\n",
    "    directory=dataset_url,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    subset=\"validation\",\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Compute class weights\n",
    "class_indices = train_dataset.class_indices\n",
    "class_labels = list(class_indices.keys())\n",
    "class_indices = list(class_indices.values())\n",
    "class_counts_dict = {class_name: len(glob.glob(os.path.join(dataset_url, class_name, '*'))) for class_name in class_labels}\n",
    "class_counts = np.array([class_counts_dict[class_name] for class_name in class_labels])\n",
    "weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(class_indices),\n",
    "    y=np.array([train_dataset.classes[i] for i in range(len(train_dataset.classes))])\n",
    ")\n",
    "class_weight_dict = dict(zip(class_labels, weights))\n",
    "\n",
    "# Define the ResNet50 model\n",
    "def identity_block(X, f, filters, training=True, initializer=glorot_uniform):\n",
    "    F1, F2, F3 = filters\n",
    "    X_shortcut = X\n",
    "    X = Conv2D(filters=F1, kernel_size=1, strides=(1, 1), padding='valid', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "    X = Add()([X_shortcut, X])\n",
    "    X = Activation('relu')(X, training=training)\n",
    "    return X\n",
    "\n",
    "def convolutional_block(X, f, filters, s=2, training=True, initializer=glorot_uniform):\n",
    "    F1, F2, F3 = filters\n",
    "    X_shortcut = X\n",
    "    X = Conv2D(filters=F1, kernel_size=1, strides=(s, s), padding='valid', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(F2, (f, f), strides=(1, 1), padding='same', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(F3, (1, 1), strides=(1, 1), padding='valid', kernel_initializer=initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X, training=training)\n",
    "    X_shortcut = Conv2D(F3, (1, 1), strides=(s, s), padding='valid', kernel_initializer=initializer(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3)(X_shortcut, training=training)\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    return X\n",
    "\n",
    "def ResNet50(input_shape=(img_height, img_width, 3), classes=len(class_labels)):\n",
    "    X_input = Input(input_shape)\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    X = Conv2D(64, (7, 7), strides=(2, 2), kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    X = convolutional_block(X, f=3, filters=[64, 64, 256], s=1)\n",
    "    X = identity_block(X, 3, [64, 64, 256])\n",
    "    X = identity_block(X, 3, [64, 64, 256])\n",
    "    X = convolutional_block(X, f=3, filters=[128, 128, 512], s=2)\n",
    "    X = identity_block(X, 3, [128, 128, 512])\n",
    "    X = identity_block(X, 3, [128, 128, 512])\n",
    "    X = identity_block(X, 3, [128, 128, 512])\n",
    "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], s=2)\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], s=2)\n",
    "    X = identity_block(X, 3, [512, 512, 2048])\n",
    "    X = identity_block(X, 3, [512, 512, 2048])\n",
    "    X = AveragePooling2D(pool_size=(2, 2), name='avg_pool')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', kernel_initializer=glorot_uniform(seed=0))(X)\n",
    "    model = Model(inputs=X_input, outputs=X)\n",
    "    return model\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model_path = r'C:\\Users\\abudh\\Desktop\\CropWatch'\n",
    "\n",
    "\n",
    "# Corrected file paths to end with `.weights.h5`\n",
    "dynamic_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=model_path + r'\\Training_model_epoch_13_08{epoch:02d}.weights.h5',  # Use .weights.h5 extension\n",
    "    save_freq='epoch',\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=model_path + r'\\best_weights_13_08.weights.h5',  # Use .weights.h5 extension\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Compile and train the model\n",
    "model = ResNet50(input_shape=(img_height, img_width, 3), classes=len(class_labels))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=no_epochs,\n",
    "    validation_data=test_dataset,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[reduce_lr, early_stopping, dynamic_checkpoint_callback, best_model_checkpoint_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b0db6b-de7d-4145-a24d-4e96f4947aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "\n",
    "model_path = r'C:\\Users\\abudh\\Desktop\\CropWatch\\13_08_model.h5'\n",
    "model.save(model_path)\n",
    "\n",
    "\n",
    "# Convert model to TensorFlow Lite format\n",
    "converter = tflite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "tflite_model_path = r'C:\\Users\\abudh\\Desktop\\CropWatch\\13_08_model.tflite'\n",
    "\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"Model converted to TFLite format and saved at: {tflite_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac7f9f-af1b-488e-8427-b3850eee0641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Perform inference with TFLite model (example)\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()[0]\n",
    "output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "def predict_image_tflite(image):\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image.astype(np.float32)\n",
    "    \n",
    "    interpreter.set_tensor(input_details['index'], image)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    output_data = interpreter.get_tensor(output_details['index'])\n",
    "    return np.argmax(output_data, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b902b-7e40-4a5c-9567-9a386a1090d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function for predicting with TFLite model\n",
    "def predict_image_tflite(image):\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = image.astype(np.float32)\n",
    "    \n",
    "    interpreter.set_tensor(input_details['index'], image)\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    output_data = interpreter.get_tensor(output_details['index'])\n",
    "    return np.argmax(output_data, axis=1)\n",
    "\n",
    "# Initialize lists for storing predictions and true labels\n",
    "y_pred = []  # store predicted labels\n",
    "y_true = []  # store true labels\n",
    "\n",
    "# Iterate over the dataset\n",
    "num_batches = 10  # Limit to a smaller number for faster feedback\n",
    "for i, (image_batch, label_batch) in enumerate(test_dataset):\n",
    "    if i >= num_batches:\n",
    "        break\n",
    "    # Append true labels\n",
    "    y_true.extend(np.argmax(label_batch, axis=1))\n",
    "    # Compute predictions with TFLite model\n",
    "    preds = [predict_image_tflite(image) for image in image_batch]\n",
    "    # Append predicted labels\n",
    "    y_pred.extend(preds)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Normalize the confusion matrix by converting counts to percentages\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "# Define class names for plotting\n",
    "class_indices = train_dataset.class_indices  # Ensure this is a dictionary\n",
    "class_names = list(class_indices.keys())\n",
    "\n",
    "# Plot the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Normalized Confusion Matrix (Percentage)')\n",
    "plt.show()\n",
    "\n",
    "# Print number of correct predictions and false positives\n",
    "correct_predictions = np.diag(cm)\n",
    "total_correct = correct_predictions.sum()\n",
    "\n",
    "false_positives = cm.sum(axis=0) - correct_predictions\n",
    "total_false_positives = false_positives.sum()\n",
    "\n",
    "print(f\"Number of correct predictions per class: {correct_predictions}\")\n",
    "print(f\"Total number of correct predictions: {total_correct}\")\n",
    "print(f\"Number of false positives per class: {false_positives}\")\n",
    "print(f\"Total number of false positives: {total_false_positives}\")\n",
    "\n",
    "# Print a sample of predictions and true labels for inspection\n",
    "print(f\"Sample true labels: {y_true[:10]}\")\n",
    "print(f\"Sample predicted labels: {y_pred[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd043fb-4b6c-4962-a26c-17c36cb1bfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
